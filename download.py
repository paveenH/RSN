from huggingface_hub import snapshot_download
import os

# 1. Configure paths
model_id = "meta-llama/Llama-3.3-70B-Instruct"
local_dir = "/work/d12922004/models/Llama-3.3-70B-Instruct"
cache_dir = "/work/d12922004/hf_cache"

# 2. Ensure directories exist
os.makedirs(local_dir, exist_ok=True)
os.makedirs(cache_dir, exist_ok=True)

print(f"ğŸš€ å¼€å§‹ä¸‹è½½ {model_id}...")
print(f"ğŸ“‚ ç›®æ ‡ç›®å½•: {local_dir}")
print(f"ğŸ“¦ ç¼“å­˜ç›®å½•: {cache_dir}")

try:
    # 3. Perform download
    snapshot_download(
        repo_id=model_id,
        local_dir=local_dir,
        cache_dir=cache_dir,
        local_dir_use_symlinks=True, # Use symlinks to save space
        token=True,                   # Will automatically read your previously logged-in token
        max_workers=8                 # Enable multithreaded download
    )
    print("\nâœ… ä¸‹è½½æˆåŠŸï¼æ¨¡å‹å·²å°±ä½ã€‚")
except Exception as e:
    print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
    print("ğŸ’¡ æç¤ºï¼šè¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡ 'huggingface-cli login' æˆ–è€…è®¾ç½®äº† HF_TOKEN ç¯å¢ƒå˜é‡ã€‚")