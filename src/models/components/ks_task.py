#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb  4 20:07:37 2025

This script computes the KS test (Kolmogorov-Smirnov) for each neuron
based on inconsistent samples between Expert and None-Expert for each task.
For each task, a KS statistic matrix (of shape: (num_layers, hidden_size))
is computed. Then, for each task a binary mask is generated by selecting
the top (hidden_size // 200) neurons per layer (excluding the embedding layer)
based on the KS statistic. Finally, the Dice coefficient is computed for each pair
of tasks, yielding a Dice similarity matrix (shape: (num_tasks, num_tasks)) which is saved.
Neurons with low p-values (< threshold) are considered "role-sensitive", and a
high KS statistic indicates a larger distributional difference.

@author: paveenhuang
"""


import os
import numpy as np
import json
import argparse
from scipy.stats import ks_2samp
import sys

if os.fork():
    sys.exit()  

# -------------------------------
# Parse command-line arguments
# -------------------------------
parser = argparse.ArgumentParser(description="Compute KS test for neurons based on inconsistent samples of expert and non-expert for each task, and then compute Dice similarity between tasks")
parser.add_argument("model", type=str, help="Name of the model (e.g., llama3)")
parser.add_argument("size", type=str, help="Size of the model (e.g., 1B)")
# p-value threshold in the original description; 可根据需要调整
parser.add_argument("p_threshold", type=float, default=0.05, help="p-value threshold for significance in KS test")
args = parser.parse_args()

model = args.model
size = args.size
p_threshold = args.p_threshold

# model = "llama3"
# size = "3B"

# -------------------------------
# Path setup
# -------------------------------
current_path = os.getcwd()
hidden_states_path = os.path.join(current_path, "hidden_states_v3", model)
json_path = os.path.join(current_path, "answer", model)
save_path = os.path.join(current_path, "ks_test_results", model)
os.makedirs(save_path, exist_ok=True)

# -------------------------------
# Task list (e.g., 57 tasks)
# -------------------------------
TASKS = [
    "abstract_algebra", "anatomy", "astronomy", 
    "business_ethics", "clinical_knowledge", 
    "college_biology", "college_chemistry", "college_computer_science", "college_medicine", 
    "college_mathematics", "college_physics", "computer_security", "conceptual_physics", 
    "econometrics", "electrical_engineering", "elementary_mathematics", "formal_logic", 
    "global_facts", "high_school_biology", "high_school_chemistry", "high_school_computer_science", 
    "high_school_european_history", "high_school_geography", "high_school_government_and_politics", 
    "high_school_macroeconomics", "high_school_mathematics", "high_school_microeconomics", 
    "high_school_physics", "high_school_psychology", "high_school_statistics", 
    "high_school_us_history", "high_school_world_history", "human_aging", "human_sexuality", 
    "international_law", "jurisprudence", "logical_fallacies", "machine_learning", "management", 
    "marketing", "medical_genetics", "miscellaneous", "moral_disputes", "moral_scenarios", 
    "nutrition", "philosophy", "prehistory", "professional_accounting", "professional_law", 
    "professional_medicine", "professional_psychology", "public_relations", "security_studies", 
    "sociology", "us_foreign_policy", "virology", "world_religions"
]

ks_statistics_tasks = {}
p_values_tasks = {}

# -------------------------------
# Process each task: compute KS test per neuron for each task separately
# -------------------------------
for task in TASKS:
    print(f"\nProcessing task: {task}")
    
    char_filepath = os.path.join(hidden_states_path, f"{task}_{task}_{size}.npy")
    none_char_filepath = os.path.join(hidden_states_path, f"none_{task}_{task}_{size}.npy")
    json_filepath = os.path.join(json_path, f"{task}_{size}_answers.json")
    
    if not os.path.exists(char_filepath):
        print(f"Char hidden states file not found for task: {task}, skipping.")
        continue
    if not os.path.exists(none_char_filepath):
        print(f"None‑char hidden states file not found for task: {task}, skipping.")
        continue
    if not os.path.exists(json_filepath):
        print(f"JSON file not found for task: {task}, skipping.")
        continue
    
    try:
        char_data = np.load(char_filepath)
        none_char_data = np.load(none_char_filepath)
    except Exception as e:
        print(f"Error loading hidden states for task {task}: {e}")
        continue

    if char_data.ndim == 4:
        char_data = np.squeeze(char_data, axis=1)
    if none_char_data.ndim == 4:
        none_char_data = np.squeeze(none_char_data, axis=1)
    
    try:
        with open(json_filepath, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
    except Exception as e:
        print(f"Error loading JSON for task {task}: {e}")
        continue
    
    inconsistent_indices = []
    for idx, entry in enumerate(json_data.get("data", [])):
        ans_expert = entry.get(f"answer_{task}")
        ans_none = entry.get(f"answer_none_{task}")
        if ans_expert != ans_none:
            inconsistent_indices.append(idx)
    
    if not inconsistent_indices:
        print(f"No inconsistent samples found for task: {task}, skipping.")
        continue
    
    if max(inconsistent_indices) >= char_data.shape[0] or max(inconsistent_indices) >= none_char_data.shape[0]:
        valid_indices = [i for i in inconsistent_indices if i < char_data.shape[0] and i < none_char_data.shape[0]]
        if not valid_indices:
            continue
        expert_data_diff = char_data[valid_indices, ...]
        none_expert_data_diff = none_char_data[valid_indices, ...]
    else:
        expert_data_diff = char_data[inconsistent_indices, ...]
        none_expert_data_diff = none_char_data[inconsistent_indices, ...]
    
    print(f"Task {task}: total samples = {char_data.shape[0]}, inconsistent samples used = {expert_data_diff.shape[0]}")
    
    expert_data_diff = expert_data_diff.astype(np.float64)
    none_expert_data_diff = none_expert_data_diff.astype(np.float64)
    expert_data_diff = np.clip(expert_data_diff, -1e6, 1e6)
    none_expert_data_diff = np.clip(none_expert_data_diff, -1e6, 1e6)
    
    task_num_layers, task_hidden_size = expert_data_diff.shape[1], expert_data_diff.shape[2]
    ks_stat_matrix = np.zeros((task_num_layers, task_hidden_size), dtype=np.float64)
    p_val_matrix = np.ones((task_num_layers, task_hidden_size), dtype=np.float64)
    
    for layer in range(task_num_layers):
        for neuron in range(task_hidden_size):
            expert_activations = expert_data_diff[:, layer, neuron]
            none_expert_activations = none_expert_data_diff[:, layer, neuron]
            statistic, p_value = ks_2samp(expert_activations, none_expert_activations)
            ks_stat_matrix[layer, neuron] = statistic
            p_val_matrix[layer, neuron] = p_value

    ks_statistics_tasks[task.replace('_', ' ')] = ks_stat_matrix
    p_values_tasks[task.replace('_', ' ')] = p_val_matrix

if not ks_statistics_tasks:
    raise ValueError("No valid inconsistent samples found across tasks.")

sorted_tasks = sorted(ks_statistics_tasks.keys())
print(f"Sorted tasks: {sorted_tasks}")

final_ks_matrix = np.array([ks_statistics_tasks[task] for task in sorted_tasks])
final_p_matrix = np.array([p_values_tasks[task] for task in sorted_tasks])
print(f"Final KS statistics matrix shape: {final_ks_matrix.shape}")
print(f"Final p-values matrix shape: {final_p_matrix.shape}")

# # Save KS
# ks_save_path = os.path.join(save_path, f"ks_statistics_inconsistent_{size}.npy")
# np.save(ks_save_path, final_ks_matrix)
# print(f"KS statistics saved to: {ks_save_path}")

# p_save_path = os.path.join(save_path, f"ks_p_values_inconsistent_{size}.npy")
# np.save(p_save_path, final_p_matrix)
# print(f"KS p-values saved to: {p_save_path}")

# ----------------------------------------------------------------------------
# Dice coefficient 
# ----------------------------------------------------------------------------

def dice_coefficient(A, B):
    """
    Compute the Dice (Sørensen–Dice) coefficient between two binary masks A and B.
    A and B should be numpy arrays containing 0s and 1s.
    """
    intersection = np.sum(A * B)
    sum_A = np.sum(A)
    sum_B = np.sum(B)
    if sum_A + sum_B == 0:
        return 1.0
    return 2 * intersection / (sum_A + sum_B)

num_tasks, num_layers, hidden_size = final_ks_matrix.shape
masks = []

for t in range(num_tasks):
    mask_task = np.zeros((num_layers, hidden_size))
    for layer in range(1, num_layers):
        layer_values = final_ks_matrix[t, layer, :]  # KS statistic 值（非负）
        top_n = max(hidden_size // 200, 1)
        top_indices = np.argsort(layer_values)[-top_n:]
        mask_layer = np.zeros(hidden_size)
        mask_layer[top_indices] = 1
        mask_task[layer, :] = mask_layer
    masks.append(mask_task)

masks = np.array(masks)  # shape: (num_tasks, num_layers, hidden_size)

dice_matrix = np.zeros((num_tasks, num_tasks))
for i in range(num_tasks):
    for j in range(num_tasks):
        dice_matrix[i, j] = dice_coefficient(masks[i].flatten(), masks[j].flatten())

print(f"Dice similarity matrix shape: {dice_matrix.shape}")

dice_save_path = os.path.join(save_path, f"dice_ks_inconsistent_{size}.npy")
np.save(dice_save_path, dice_matrix)
print(f"Dice similarity matrix saved to: {dice_save_path}")